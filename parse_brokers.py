#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§£æè­‰åˆ¸å•†åŸºæœ¬è³‡æ–™ä¸¦çµ±è¨ˆåˆ†æ”¯æ•¸é‡ï¼Œ
ä¸¦ç”Ÿæˆå°æ‡‰çš„ç°¡ç¨±åº« (branch_full_name â†’ parent_company æ˜ å°„)

ä½¿ç”¨æ–¹å¼:
  python parse_brokers.py è­‰åˆ¸å•†åŸºæœ¬è³‡æ–™.csv

è¼¸å‡ºæª”æ¡ˆ:
  branch_counts.csv      åˆ†æ”¯çµ±è¨ˆçµæœ (parent_company, branch_count)
  abbreviation_map.json  ç°¡ç¨±åº« (branch_full_name â†’ parent_company)
"""

import pandas as pd
import argparse
import os
import json
import sys
import re
import datetime
import numpy as np

# è‡ªè¨‚æ¯å…¬å¸ç°¡ç¨±å°ç…§å­—å…¸
CUSTOM_ABBREV = {
    "ä¸­åœ‹ä¿¡è¨—": "ä¸­ä¿¡",
    "è‡ºç£ä¼éŠ€": "å°ä¼",
    "å°ç£ä¼éŠ€": "å°ä¼",
    "åœŸåœ°éŠ€è¡Œ": "åœŸéŠ€",
    "è‡ºéŠ€è­‰åˆ¸": "è‡ºéŠ€",
    "åˆä½œé‡‘åº«": "åˆåº«",
    "å½°åŒ–éŠ€è¡Œ": "å½°éŠ€",
    "ç¬¬ä¸€é‡‘": "ç¬¬ä¸€è­‰",
    "çµ±ä¸€è­‰åˆ¸": "çµ±ä¸€",
    "å…ƒå¯Œè­‰åˆ¸": "å…ƒå¯Œ",
    "å°ä¸­éŠ€è¡Œ": "å°ä¸­éŠ€",
    "å…†è±è­‰åˆ¸": "å…†è±",
    "åœ‹æ³°è­‰åˆ¸": "åœ‹æ³°",
    "ç¾¤ç›Šé‡‘é¼": "ç¾¤ç›Š",
    "å‡±åŸºè­‰åˆ¸": "å‡±åŸº",
    "è¯å—æ°¸æ˜Œ": "è¯å—",
    "å¯Œé‚¦è­‰åˆ¸": "å¯Œé‚¦",
    "å…ƒå¤§è­‰åˆ¸": "å…ƒå¤§",
    "æ°¸è±é‡‘": "æ°¸è±",
    "ç‰å±±è­‰åˆ¸": "ç‰å±±",
    "æ–°å…‰è­‰åˆ¸": "æ–°å…‰",
    "é™½ä¿¡è­‰åˆ¸": "é™½ä¿¡",
    "è¯é‚¦è­‰åˆ¸": "è¯é‚¦",
    "åº·å’Œè­‰åˆ¸": "åº·å’Œ",
    "å°æ–°è­‰åˆ¸": "å°æ–°",
    "åœ‹ç¥¨è­‰åˆ¸": "åœ‹ç¥¨",
    "äºæ±è­‰åˆ¸": "äºæ±",
    "è‡´å’Œè­‰åˆ¸": "è‡´å’Œ",
    "å®‰æ³°è­‰åˆ¸": "å®‰æ³°",
    "ç¦é‚¦è­‰åˆ¸": "ç¦é‚¦",
    "æ°¸å…¨è­‰åˆ¸": "æ°¸å…¨",
    "å¤§æ˜Œè­‰åˆ¸": "å¤§æ˜Œ",
    "å¾·ä¿¡è­‰åˆ¸": "å¾·ä¿¡",
    "ç¦å‹è­‰åˆ¸": "ç¦å‹",
    "å…‰å’Œè­‰åˆ¸": "å…‰å’Œ",
    "æ–°ç™¾ç‹è­‰åˆ¸": "æ–°ç™¾ç‹",
    "é«˜æ©‹è­‰åˆ¸": "é«˜æ©‹",
    "ç¾å¥½è­‰åˆ¸": "ç¾å¥½",
    "å¤§å±•è­‰åˆ¸": "å¤§å±•",
    "å¯Œéš†è­‰åˆ¸": "å¯Œéš†",
    "ç›ˆæº¢è­‰åˆ¸": "ç›ˆæº¢",
    "å¯¶ç››è­‰åˆ¸": "å¯¶ç››",
    "æ°¸èˆˆè­‰åˆ¸": "æ°¸èˆˆ",
    "æ—¥é€²è­‰åˆ¸": "æ—¥é€²",
    "æ—¥èŒ‚è­‰åˆ¸": "æ—¥èŒ‚",
    "çŠ‡äºè­‰åˆ¸": "çŠ‡äº",
    "åŒ—åŸè­‰åˆ¸": "åŒ—åŸ",
    "çŸ³æ©‹è­‰åˆ¸": "çŸ³æ©‹",
    "ä¸­è¾²è­‰åˆ¸": "ä¸­è¾²",
    "å£è¢‹è­‰åˆ¸": "å£è¢‹",
    "äº¬åŸè­‰åˆ¸": "äº¬åŸ",
    "å®é è­‰åˆ¸": "å®é ",
    "å…ƒå¤§æœŸè²¨": "å…ƒå¤§",
    "ç¾¤ç›ŠæœŸè²¨": "ç¾¤ç›Š",
    "æ¸¯å•†éº¥æ ¼ç†": "éº¥æ ¼ç†",
    "å°ç£åŒ¯ç«‹": "åŒ¯ç«‹",
    "ç¾æ—": "ç¾æ—",
    "å°ç£æ‘©æ ¹å£«ä¸¹åˆ©": "æ‘©æ ¹å£«ä¸¹åˆ©",
    "ç¾å•†é«˜ç››": "é«˜ç››",
    "æ¸¯å•†é‡æ‘": "é‡æ‘",
    "æ¸¯å•†æ³•åœ‹èˆˆæ¥­": "æ³•åœ‹èˆˆæ¥­",
    "èŠ±æ——ç’°çƒ": "èŠ±æ——",
    "æ–°åŠ å¡å•†ç‘éŠ€": "ç‘éŠ€",
    "å¤§å’Œåœ‹æ³°": "å¤§å’Œåœ‹æ³°",
    "æ³•éŠ€å·´é»": "æ³•éŠ€å·´é»",
    "é¦™æ¸¯ä¸Šæµ·åŒ¯è±": "åŒ¯è±",
    "æ‘©æ ¹å¤§é€š": "æ‘©æ ¹å¤§é€š"
}

def normalize_name(name):
    """æ¨™æº–åŒ–åç¨±ï¼šç§»é™¤å¤šé¤˜ç©ºæ ¼ã€çµ±ä¸€åˆ†éš”ç¬¦"""
    if not isinstance(name, str):
        return str(name).strip()
    return re.sub(r'\s+', ' ', name.replace('ï¼', '-').strip())

def extract_parent_name(full_name):
    """å¾å®Œæ•´åˆ†è¡Œåç¨±æå–æ¯å…¬å¸åç¨±"""
    full_name = normalize_name(full_name)
    # åˆ†å‰²ç¬¦è™Ÿï¼š- æˆ– ç©ºæ ¼ï¼Œåƒ…åˆ†å‰²ä¸€æ¬¡
    parts = re.split(r'[- ]', full_name, maxsplit=1)
    parent = parts[0].strip()
    return parent

# å‡½æ•¸ç”¨æ–¼æ¸…ç†å’Œè§£ææ™‚é–“æˆ³
def parse_timestamp(timestamp):
    try:
        # è™•ç†æ™‚é–“æˆ³è¨˜æ ¼å¼
        if not isinstance(timestamp, str):
            return pd.NaT
        
        # æ¸…ç†æ™‚é–“æˆ³ä¸¦è½‰ç‚ºISO8601æ ¼å¼
        timestamp = timestamp.strip()
        
        # è™•ç† "2025/5/16 ä¸Šåˆ 10:20:44" ä¹‹é¡çš„æ ¼å¼
        if 'ä¸Šåˆ' in timestamp:
            timestamp = timestamp.replace('ä¸Šåˆ', 'AM')
        elif 'ä¸‹åˆ' in timestamp:
            timestamp = timestamp.replace('ä¸‹åˆ', 'PM')
        
        # å˜—è©¦å¤šç¨®æ™‚é–“æ ¼å¼è§£æ
        try:
            dt = pd.to_datetime(timestamp, format='%Y/%m/%d %I:%M:%S')
        except:
            try:
                dt = pd.to_datetime(timestamp, format='%Y/%m/%d %H:%M:%S')
            except:
                try:
                    dt = pd.to_datetime(timestamp, format='%Y/%m/%d %p %I:%M:%S')
                except:
                    dt = pd.to_datetime(timestamp)
        
        # è½‰æ›ç‚ºISO8601æ ¼å¼
        return dt.strftime('%Y-%m-%dT%H:%M:%S+08:00')
    except:
        return pd.NaT

# å‡½æ•¸ç”¨æ–¼è©•åˆ†æ¯ç­†è³‡æ–™çš„æƒ…ç·’/å£“è¿«æŒ‡æ¨™
def score_entry(dialog, inner_thought):
    # åˆå§‹åŒ–è©•åˆ†
    scores = {}
    
    # åˆä½µå°è©±å’Œå…§å¿ƒè©±é€²è¡Œè©•åˆ†
    combined_text = dialog + " " + inner_thought if inner_thought else dialog
    
    # ç„¡å¥ˆæ„Ÿï¼šè¢«è¿«ã€å¿ƒåŠ›äº¤ç˜ç¨‹åº¦
    helplessness = 0
    helplessness_words = ["ç„¡å¥ˆ", "è¢«è¿«", "å¿ƒåŠ›äº¤ç˜", "æ²’è¾¦æ³•", "ç„¡æ³•", "åªèƒ½", "åªå¥½", "ä¸å¾—ä¸", "è¿«ä¸å¾—å·²", "å¿ƒç´¯", "æ²’å¾—é¸", "ç„¡åŠ›"]
    helplessness += sum([3 for word in helplessness_words if word in combined_text])
    helplessness += 3 if "è‡ªå·±è²·" in combined_text else 0
    helplessness += 3 if "è‡ªæè…°åŒ…" in combined_text else 0
    helplessness += 2 if "è–ªæ°´" in combined_text and ("è²·" in combined_text or "åšæ¥­ç¸¾" in combined_text) else 0
    scores["ç„¡å¥ˆæ„Ÿ (1-10)"] = min(10, max(1, helplessness))
    
    # è¢«æ¬ºå£“æ„Ÿï¼šæ¬ŠåŠ›ä¸å°ç­‰å£“è¿«ç¨‹åº¦
    oppression = 0
    oppression_words = ["æ¬ºå£“", "å£“è¿«", "å¨è„…", "æåš‡", "å¼·è¿«", "é€¼è¿«", "å¨è„…", "éœ¸å‡Œ", "æ¿«ç”¨æ¬ŠåŠ›", "å¿…é ˆ", "æƒ³è¾¦æ³•", "ä½¿å‘½å¿…é”"]
    oppression += sum([2 for word in oppression_words if word in combined_text])
    oppression += 3 if "è²¬ä»»é¡" in combined_text and ("è¦é”æˆ" in combined_text or "è¦åšåˆ°" in combined_text) else 0
    oppression += 3 if "å®¢æˆ¶" in combined_text and "ç§»è½‰" in combined_text else 0
    oppression += 3 if "è¾¦å…¬å®¤" in combined_text and "ç´„è«‡" in combined_text else 0
    oppression += 3 if "åšä¸åˆ°" in combined_text and "é›¢è·" in combined_text else 0
    scores["è¢«æ¬ºå£“æ„Ÿ (1-10)"] = min(10, max(1, oppression))
    
    # ç¾è¾±æ„Ÿï¼šäººæ ¼è²¶ä½ã€å˜²è«·ç¨‹åº¦
    humiliation = 0
    humiliation_words = ["ç¾è¾±", "è²¶ä½", "å˜²è«·", "ä¸Ÿè‡‰", "ç¬¨è›‹", "ç„¡èƒ½", "æ²’ç”¨", "å»¢ç‰©", "æ‰“æ··", "æ“ºçˆ›", "ä¸å­¸ç„¡è¡“", "åƒåœ¾"]
    humiliation += sum([3 for word in humiliation_words if word in combined_text])
    humiliation += 3 if "åˆ¥äººéƒ½åšå¾—åˆ°" in combined_text else 0
    humiliation += 3 if "ç•¶çœ¾" in combined_text and ("ç½µ" in combined_text or "ç³Ÿ" in combined_text or "æ‰¹è©•" in combined_text) else 0
    humiliation += 4 if "è£™å­" in combined_text and "çŸ­" in combined_text else 0
    scores["ç¾è¾±æ„Ÿ (1-10)"] = min(10, max(1, humiliation))
    
    # æ€§åˆ¥æ­§è¦–æ„Ÿï¼šæ€§åˆ¥åˆ»æ¿å°è±¡èˆ‡æ­§è¦–
    gender_discrimination = 0
    gender_words = ["å¥³ç”Ÿ", "å¥³æ€§", "ç”·ç”Ÿ", "ç”·æ€§", "æ€§åˆ¥", "æ­§è¦–", "æ€§é¨·æ“¾"]
    if any(word in combined_text for word in gender_words):
        gender_discrimination += 5
    gender_discrimination += 5 if "è£™å­" in combined_text or "å¥³ç”Ÿè¦æœƒåˆ©ç”¨è‡ªå·±å„ªå‹¢" in combined_text else 0
    gender_discrimination += 4 if "ä¸€è…¿" in combined_text else 0
    gender_discrimination += 3 if "çŠ§ç‰²è‡ªå·±" in combined_text and "å¥³" in combined_text else 0
    scores["æ€§åˆ¥æ­§è¦–æ„Ÿ (1-10)"] = min(10, max(1, gender_discrimination))
    
    # ä¾®è¾±æ€§ï¼šä¾®è¾±ç”¨è©é »ç‡èˆ‡å¼·åº¦
    insult = 0
    insult_words = ["å»æ­»", "ç™½ç—´", "ç¬¨è›‹", "å»¢ç‰©", "è±¬ç‹—ä¸å¦‚", "åƒå±", "åƒåœ¾", "æ··è›‹", "ç‹å…«è›‹", "åª½çš„"]
    insult += sum([3 for word in insult_words if word in combined_text])
    insult += 3 if "æ€éº¼é€™éº¼" in combined_text and ("ç¬¨" in combined_text or "è ¢" in combined_text) else 0
    insult += 3 if "å¼" in combined_text or "å¤§è²" in combined_text else 0
    insult += 3 if re.search(r'[#ï¼ƒ*]+', combined_text) else 0
    scores["ä¾®è¾±æ€§ (1-10)"] = min(10, max(1, insult))
    
    # ææ‡¼ç„¦æ…®ï¼šå®³æ€•å¾Œæœèˆ‡ç„¦èº
    anxiety = 0
    anxiety_words = ["å®³æ€•", "ç„¦æ…®", "æ“”å¿ƒ", "å£“åŠ›", "ææ‡¼", "æ‡¼æ€•", "æ€•", "æ“¾æ°‘", "ä¸å®‰", "æ†‚æ…®", "è€ƒç¸¾", "å¹´çµ‚", "è³‡é£"]
    anxiety += sum([2 for word in anxiety_words if word in combined_text])
    anxiety += 3 if "è¾¦å…¬å®¤" in combined_text and "ç´„è«‡" in combined_text else 0
    anxiety += 3 if "è€ƒæ ¸" in combined_text else 0
    anxiety += 3 if "é”ä¸åˆ°" in combined_text and ("ä¸ä¿" in combined_text or "ç¸¾æ•ˆ" in combined_text) else 0
    anxiety += 3 if "çœ‹é†«ç”Ÿ" in combined_text or "åƒè—¥" in combined_text else 0
    scores["ææ‡¼ç„¦æ…® (1-10)"] = min(10, max(1, anxiety))
    
    # å£“è¿«æ€§æ•˜è¿°ï¼šå‘½ä»¤ã€å¼·åˆ¶æ„å‘³
    coercion = 0
    coercion_words = ["å¿…é ˆ", "ä¸€å®šè¦", "ä¸å‡†", "ä¸å…è¨±", "ä¸å¾—", "è¦æ±‚", "ä½¿å‘½å¿…é”", "æ²’æœ‰é¸æ“‡", "è²¸æ¬¾"]
    coercion += sum([2 for word in coercion_words if word in combined_text])
    coercion += 3 if "è²¬ä»»é¡" in combined_text and "100%" in combined_text else 0
    coercion += 3 if "è‡ªå·±è²·" in combined_text or "è‡ªæè…°åŒ…" in combined_text else 0
    coercion += 3 if "ä¿¡è²¸" in combined_text and "è²·" in combined_text else 0
    coercion += 3 if "ä½¿å‘½å¿…é”" in combined_text else 0
    scores["å£“è¿«æ€§æ•˜è¿° (1-10)"] = min(10, max(1, coercion))
    
    # æƒ…ç·’çˆ†é»å€¼ï¼šå…·æœ‰æ–°èçˆ†é»çš„æªè¾­
    emotional_peak = 0
    peak_words = ["è©é¨™", "é•æ³•", "çŠ¯ç½ª", "ä¸ç•¶", "é¨™", "PUA", "é»‘é“", "è·å ´éœ¸å‡Œ", "ç”Ÿç—…", "å´©æ½°"]
    emotional_peak += sum([2 for word in peak_words if word in combined_text])
    emotional_peak += 5 if "è²¸æ¬¾" in combined_text and "è²·" in combined_text and "è²¬ä»»é¡" in combined_text else 0
    emotional_peak += 5 if "è‡ªæ®º" in combined_text or "è¼•ç”Ÿ" in combined_text else 0
    emotional_peak += 4 if "è£™å­" in combined_text and "çŸ­" in combined_text else 0
    emotional_peak += 3 if "è²¼å¿ƒå¥³å­©" in combined_text or "å¥³ç”Ÿåˆ©ç”¨å„ªå‹¢" in combined_text else 0
    emotional_peak += 3 if "çŠ§ç‰²è‡ªå·±" in combined_text else 0
    scores["æƒ…ç·’çˆ†é»å€¼ (1-10)"] = min(10, max(1, emotional_peak))
    
    # å§”å±ˆæ²‰é»˜ï¼šå£“æŠ‘ã€ä¸æ•¢åé§
    silence = 0
    silence_words = ["å§”å±ˆ", "æ²‰é»˜", "ä¸æ•¢èªª", "ä¸èƒ½èªª", "æ†‹è‘—", "å¿è‘—", "é»˜é»˜", "å£“æŠ‘", "ç„¡è™•è¨´è‹¦", "è‹¦é›£è¨€"]
    silence += sum([2 for word in silence_words if word in combined_text])
    silence += 3 if "ä¸æ•¢" in combined_text else 0
    silence += 3 if "æ€•" in combined_text and "å¤±å»" in combined_text else 0
    silence += 4 if inner_thought and len(inner_thought) > len(dialog) * 2 else 0
    silence += 3 if "ä¸èƒ½èªª" in combined_text or "ä¸æ•¢è¬›" in combined_text else 0
    scores["å§”å±ˆæ²‰é»˜ (1-10)"] = min(10, max(1, silence))
    
    # èªæ°£å¼·åº¦ï¼šæƒ…æ„Ÿå¼µåŠ›å¼·åº¦
    intensity = 0
    intensity += 1 if "ï¼" in combined_text else 0
    intensity += 2 if "ï¼ï¼" in combined_text else 0
    intensity += 3 if "ï¼ï¼ï¼" in combined_text else 0
    intensity += 2 if "ï¼Ÿï¼Ÿ" in combined_text else 0
    intensity += 2 if dialog and dialog.isupper() else 0
    intensity += 3 if inner_thought and inner_thought.isupper() else 0
    intensity += 3 if re.search(r'[ã€‚]+', combined_text) else 0
    intensity += 3 if "æ€’" in combined_text or "æ°£" in combined_text else 0
    intensity += 3 if "å»æ­»" in combined_text else 0
    scores["èªæ°£å¼·åº¦ (1-10)"] = min(10, max(1, intensity))
    
    # è¨ˆç®— NLP ç¸½åˆ† (1-100)
    total_score = sum(scores.values())
    
    # æ½¤é£¾å°è©±å’Œå…§å¿ƒè©±èª
    refined_dialog = dialog.replace("ã€", "ï¼Œ").strip() if dialog else ""
    refined_inner_thought = inner_thought.replace("ã€", "ï¼Œ").strip() if inner_thought else ""
    
    # æ¿ƒç¸®é‡‘å¥
    key_quote = ""
    if "è£™å­" in combined_text and "çŸ­" in combined_text:
        key_quote = "ä¸»ç®¡ä»¥æ€§åˆ¥æ­§è¦–è¨€è«–æš—ç¤ºå¥³æ€§æ‡‰åˆ©ç”¨èº«é«”å„ªå‹¢"
    elif "è²¸æ¬¾" in combined_text and "è²·" in combined_text and "è²¬ä»»é¡" in combined_text:
        key_quote = "ä¸»ç®¡é¼“å‹µå“¡å·¥è²¸æ¬¾è³¼è²·å•†å“é”æˆè²¬ä»»é¡"
    elif "ç§»è½‰å®¢æˆ¶" in combined_text:
        key_quote = "ä¸»ç®¡å¨è„…æœªé”æ¨™è€…å°‡ç§»è½‰å®¢æˆ¶è³‡æº"
    elif "è©é¨™" in combined_text:
        key_quote = "å“¡å·¥è¢«è¿«ä½¿ç”¨ä¸ç•¶éŠ·å”®æ‰‹æ³•ï¼Œæ„Ÿè¦ºåƒè©é¨™é›†åœ˜"
    elif "IPO" in combined_text and "è‡ªå·±è²·" in combined_text:
        key_quote = "å“¡å·¥è¢«è¿«è‡ªæè…°åŒ…è³¼è²·IPOé”æˆæ¥­ç¸¾"
    elif "ä½¿å‘½å¿…é”" in combined_text:
        key_quote = "ä¸»ç®¡ä»¥ä½¿å‘½å¿…é”ç‚ºç”±å¼·åˆ¶å“¡å·¥é”æˆä¸åˆç†ç›®æ¨™"
    elif "è·å ´éœ¸å‡Œ" in combined_text:
        key_quote = "ä¸»ç®¡ä»¥è¨€èªæš´åŠ›èˆ‡å¨è„…å¯¦æ–½è·å ´éœ¸å‡Œ"
    elif "çŠ§ç‰²è‡ªå·±" in combined_text:
        key_quote = "ä¸»ç®¡æš—ç¤ºå¥³æ€§å“¡å·¥å¯ç‚ºæ¥­ç¸¾çŠ§ç‰²è‡ªå·±"
    else:
        # å¾åŸæ–‡ä¸­æå–æ¿ƒç¸®é‡‘å¥
        important_segments = re.findall(r'ã€Œ[^ã€]+ã€|ã€[^ã€]+ã€|"[^"]+"', combined_text)
        if important_segments:
            key_quote = important_segments[0].strip('ã€Œã€ã€ã€""')
        else:
            # å¦‚æœæ²’æœ‰å¼•è™Ÿå…§å®¹ï¼Œæå–æœ€å¼·çƒˆçš„å¥å­
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', combined_text)
            if sentences:
                key_quote = max(sentences, key=len).strip()
                if len(key_quote) > 30:
                    key_quote = key_quote[:30] + "..."
    
    # é©åˆå ±å°çš„çˆ†é»æ¨™é¡Œ
    headline = ""
    if "æ€§åˆ¥æ­§è¦–" in combined_text or "è£™å­" in combined_text:
        headline = "é‡‘èæ¥­çˆ†æ€§åˆ¥æ­§è¦–ï¼šä¸»ç®¡æš—ç¤ºå¥³è·å“¡ã€Œåˆ©ç”¨èº«é«”ç‰¹å¾µã€é”æˆæ¥­ç¸¾"
    elif "è²¸æ¬¾" in combined_text and "IPO" in combined_text:
        headline = "è·å ´å£“åŠ›é©šäººï¼šé‡‘èå¾æ¥­äººå“¡è¢«è¿«è²¸æ¬¾è³¼è²·IPOé”æ¨™"
    elif "ä½¿å‘½å¿…é”" in combined_text and "IPO" in combined_text:
        headline = "é‡‘èæ¥­ã€Œä½¿å‘½å¿…é”ã€æ–‡åŒ–ä¸‹çš„å“¡å·¥è‹¦ç—›ï¼šIPOè²¬ä»»é¡æˆç„¡å½¢æ·é–"
    elif "å®¢æˆ¶ç§»è½‰" in combined_text:
        headline = "é‡‘èæ¥­æ›å…‰å¨è„…æ‰‹æ®µï¼šæœªé”è²¬ä»»é¡å°±ç§»è½‰å®¢æˆ¶è³‡æº"
    elif "è©é¨™" in combined_text:
        headline = "å¾æ¥­äººå“¡å¿ƒè²ï¼šã€Œæˆ‘å€‘åƒè©é¨™é›†åœ˜ã€æ­éœ²é‡‘èæ¥­éŠ·å”®äº‚è±¡"
    elif "è²¬ä»»é¡" in combined_text and "å£“åŠ›" in combined_text:
        headline = "è²¬ä»»é¡å£“åŠ›ä¸‹çš„é‡‘èå¾æ¥­äººå“¡ï¼šæ¯æœˆéƒ½åœ¨è²·å–®éæ´»"
    elif "åƒè—¥" in combined_text or "çœ‹é†«ç”Ÿ" in combined_text:
        headline = "éåº¦éŠ·å”®å£“åŠ›è‡´é‡‘èå¾æ¥­äººå“¡å¿ƒç†å¥åº·äº®ç´…ç‡ˆ"
    else:
        headline = "é‡‘èæ¥­IPOéŠ·å”®å£“åŠ›èª¿æŸ¥ï¼šå“¡å·¥è¢«è¿«é”æˆä¸åˆç†è²¬ä»»é¡"
    
    # è¿”å›è©•åˆ†å’Œå…¶ä»–è³‡è¨Š
    return {
        **scores,
        "NLP ç¸½åˆ† (1-100)": total_score,
        "æ½¤é£¾å¾Œè©±è¡“": refined_dialog,
        "æ½¤é£¾å¾Œå…§å¿ƒè©±èª": refined_inner_thought,
        "æ¿ƒç¸®é‡‘å¥": key_quote,
        "é©åˆå ±å°çš„çˆ†é»æ¨™é¡Œ": headline
    }

def main():
    parser = argparse.ArgumentParser(
        description='è§£æè­‰åˆ¸å•†åŸºæœ¬è³‡æ–™ä¸¦çµ±è¨ˆåˆ†æ”¯æ•¸é‡èˆ‡ç”Ÿæˆç°¡ç¨±åº«',
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('csv_file', help='è­‰åˆ¸å•†åŸºæœ¬è³‡æ–™ CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--output', '-o',
                        default='branch_counts.csv',
                        help='åˆ†æ”¯çµ±è¨ˆè¼¸å‡º CSV æª”å')
    parser.add_argument('--map', '-m',
                        default='abbreviation_map.json',
                        help='ç°¡ç¨±åº«è¼¸å‡º JSON æª”å')
    args = parser.parse_args()

    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.isfile(args.csv_file):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {args.csv_file}", file=sys.stderr)
        sys.exit(1)

    try:
        # è®€å– CSVï¼ŒæŒ‡å®šç·¨ç¢¼ä¸¦è™•ç† 'å¯èƒ½çš„ BOM
        df = pd.read_csv(args.csv_file, encoding='utf-8-sig')
    except Exception as e:
        print(f"éŒ¯èª¤: ç„¡æ³•è®€å– CSV æª”æ¡ˆ {args.csv_file}: {e}", file=sys.stderr)
        sys.exit(1)

    # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ æ¬„ä½
    if df.shape[1] < 2:
        print("éŒ¯èª¤: CSV æª”æ¡ˆæ¬„ä½ä¸è¶³ï¼Œè‡³å°‘éœ€è¦è­‰åˆ¸å•†åç¨±", file=sys.stderr)
        sys.exit(1)

    # å‡è¨­ç¬¬äºŒæ¬„ç‚ºè­‰åˆ¸å•†åç¨±
    name_col = 'è­‰åˆ¸å•†åç¨±'

    # æ¸…ç†æ•¸æ“šï¼šç§»é™¤ç©ºå€¼ä¸¦æ¨™æº–åŒ–åç¨±
    df = df.dropna(subset=[name_col])
    df[name_col] = df[name_col].apply(normalize_name)

    # æå–æ¯å…¬å¸åç¨±
    df['parent_raw'] = df[name_col].apply(extract_parent_name)

    # å¥—ç”¨è‡ªè¨‚ç°¡ç¨±ï¼Œæ‰¾ä¸åˆ°å‰‡ä¿ç•™åŸå§‹åç¨±
    df['parent_company'] = df['parent_raw'].apply(
        lambda name: CUSTOM_ABBREV.get(name, name)
    )

    # è¨ˆç®—æ¯å€‹æ¯å…¬å¸çš„åˆ†æ”¯æ•¸é‡
    counts = (
        df['parent_company']
        .value_counts()
        .rename_axis('parent_company')
        .reset_index(name='branch_count')
        .sort_values(by=['branch_count', 'parent_company'], ascending=[False, True])
    )

    # è¼¸å‡ºåˆ†æ”¯çµ±è¨ˆ CSV
    try:
        counts.to_csv(args.output, index=False, encoding='utf-8-sig')
        print(f"å·²å°‡åˆ†æ”¯çµ±è¨ˆå„²å­˜è‡³ {args.output}")
    except Exception as e:
        print(f"éŒ¯èª¤: ç„¡æ³•å¯«å…¥ {args.output}: {e}", file=sys.stderr)
        sys.exit(1)

    # ç”Ÿæˆç°¡ç¨±åº«ï¼šbranch_full_name â†’ parent_company
    mapping = dict(
        zip(
            df[name_col],
            df['parent_company']
        )
    )

    # è¼¸å‡ºç°¡ç¨±åº« JSON
    try:
        with open(args.map, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
        print(f"å·²å°‡ç°¡ç¨±åº«å„²å­˜è‡³ {args.map}")
    except Exception as e:
        print(f"éŒ¯èª¤: ç„¡æ³•å¯«å…¥ {args.map}: {e}", file=sys.stderr)
        sys.exit(1)

    # åˆ—å°æ¯å…¬å¸ç°¡ç¨±åˆ—è¡¨
    print("\n# æ¯å…¬å¸ç°¡ç¨±åˆ—è¡¨ (ä¾åˆ†æ”¯æ•¸é‡æ’åº):")
    for _, row in counts.iterrows():
        print(f"- {row['parent_company']} ({row['branch_count']} é–“)")

    # è™•ç†è³‡æ–™ï¼ˆå‡è¨­CSVæ ¼å¼ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œï¼‰
    headers = ["æ™‚é–“æˆ³è¨˜", "è·å ´å£“åŠ›è©±è¡“ï¼å…§éƒ¨å°è©±ç‰‡æ®µ", "ä½ å…§å¿ƒçœŸæ­£æƒ³èªªçš„è©±"]
    rows = []
    
    for line in data_lines:
        parts = line.split(",", 2)  # åªåˆ†å‰²å‰å…©å€‹é€—è™Ÿ
        if len(parts) >= 3:
            rows.append(parts)
        elif len(parts) == 2:
            rows.append([parts[0], parts[1], ""])
    
    # å‰µå»º DataFrame
    df = pd.DataFrame(rows, columns=headers)
    
    # æ¸…ç†å’Œè™•ç†è³‡æ–™
    df = df.drop_duplicates()
    df = df.dropna(how='all')
    
    # æ ¼å¼åŒ–æ™‚é–“æˆ³è¨˜
    df["æ™‚é–“æˆ³è¨˜"] = df["æ™‚é–“æˆ³è¨˜"].apply(parse_timestamp)
    
    # é‡å‘½ååˆ—å
    df = df.rename(columns={
        "æ™‚é–“æˆ³è¨˜": "æ™‚é–“æˆ³è¨˜",
        "è·å ´å£“åŠ›è©±è¡“ï¼å…§éƒ¨å°è©±ç‰‡æ®µ": "åŸå§‹è©±è¡“ï¼å…§éƒ¨å°è©±",
        "ä½ å…§å¿ƒçœŸæ­£æƒ³èªªçš„è©±": "åŸå§‹å…§å¿ƒè©±èª"
    })
    
    # å°æ¯ç­†è³‡æ–™é€²è¡Œè©•åˆ†
    results = []
    for _, row in df.iterrows():
        dialog = row["åŸå§‹è©±è¡“ï¼å…§éƒ¨å°è©±"] if pd.notna(row["åŸå§‹è©±è¡“ï¼å…§éƒ¨å°è©±"]) else ""
        inner_thought = row["åŸå§‹å…§å¿ƒè©±èª"] if pd.notna(row["åŸå§‹å…§å¿ƒè©±èª"]) else ""
        
        scores = score_entry(dialog, inner_thought)
        results.append({
            "æ™‚é–“æˆ³è¨˜": row["æ™‚é–“æˆ³è¨˜"],
            "åŸå§‹è©±è¡“ï¼å…§éƒ¨å°è©±": dialog,
            "åŸå§‹å…§å¿ƒè©±èª": inner_thought,
            **scores
        })
    
    # å‰µå»ºçµæœ DataFrame
    result_df = pd.DataFrame(results)
    
    # è¨­å®šæ¬„ä½é †åº
    column_order = [
        "æ™‚é–“æˆ³è¨˜", "åŸå§‹è©±è¡“ï¼å…§éƒ¨å°è©±", "åŸå§‹å…§å¿ƒè©±èª", "æ½¤é£¾å¾Œè©±è¡“", "æ½¤é£¾å¾Œå…§å¿ƒè©±èª",
        "ç„¡å¥ˆæ„Ÿ (1-10)", "è¢«æ¬ºå£“æ„Ÿ (1-10)", "ç¾è¾±æ„Ÿ (1-10)", "æ€§åˆ¥æ­§è¦–æ„Ÿ (1-10)",
        "ä¾®è¾±æ€§ (1-10)", "ææ‡¼ç„¦æ…® (1-10)", "å£“è¿«æ€§æ•˜è¿° (1-10)", "æƒ…ç·’çˆ†é»å€¼ (1-10)",
        "å§”å±ˆæ²‰é»˜ (1-10)", "èªæ°£å¼·åº¦ (1-10)", "NLP ç¸½åˆ† (1-100)",
        "æ¿ƒç¸®é‡‘å¥", "é©åˆå ±å°çš„çˆ†é»æ¨™é¡Œ"
    ]
    result_df = result_df[column_order]
    
    # è¼¸å‡º CSV æª”æ¡ˆ
    result_df.to_csv("IPO.csv", index=False)
    
    # ç”Ÿæˆ Markdown å ±å‘Š
    generate_markdown_report(result_df)
    
    print("åˆ†æå®Œæˆï¼Œå·²è¼¸å‡º IPO.csv å’Œ IPO.md æª”æ¡ˆ")

def generate_markdown_report(df):
    # è¨ˆç®—åŸºæœ¬çµ±è¨ˆæ•¸æ“š
    total_entries = len(df)
    avg_score = round(df["NLP ç¸½åˆ† (1-100)"].mean(), 1)
    
    # æ‰¾å‡ºæœ€é«˜åˆ†å’Œæœ€ä½åˆ†çš„æ¡ˆä¾‹
    highest_score_row = df.loc[df["NLP ç¸½åˆ† (1-100)"].idxmax()]
    lowest_score_row = df.loc[df["NLP ç¸½åˆ† (1-100)"].idxmin()]
    
    # è¨ˆç®—æœ€å¸¸è¦‹æƒ…ç·’åˆ†é¡
    emotion_columns = [
        "ç„¡å¥ˆæ„Ÿ (1-10)", "è¢«æ¬ºå£“æ„Ÿ (1-10)", "ç¾è¾±æ„Ÿ (1-10)", "æ€§åˆ¥æ­§è¦–æ„Ÿ (1-10)",
        "ä¾®è¾±æ€§ (1-10)", "ææ‡¼ç„¦æ…® (1-10)", "å£“è¿«æ€§æ•˜è¿° (1-10)", "æƒ…ç·’çˆ†é»å€¼ (1-10)",
        "å§”å±ˆæ²‰é»˜ (1-10)", "èªæ°£å¼·åº¦ (1-10)"
    ]
    
    emotion_map = {
        "ç„¡å¥ˆæ„Ÿ (1-10)": "ç„¡å¥ˆæ„Ÿ",
        "è¢«æ¬ºå£“æ„Ÿ (1-10)": "è¢«æ¬ºå£“æ„Ÿ",
        "ç¾è¾±æ„Ÿ (1-10)": "ç¾è¾±æ„Ÿ",
        "æ€§åˆ¥æ­§è¦–æ„Ÿ (1-10)": "æ€§åˆ¥æ­§è¦–æ„Ÿ",
        "ä¾®è¾±æ€§ (1-10)": "ä¾®è¾±æ€§è¨€èª",
        "ææ‡¼ç„¦æ…® (1-10)": "ææ‡¼ç„¦æ…®",
        "å£“è¿«æ€§æ•˜è¿° (1-10)": "å£“è¿«æ€§æ•˜è¿°",
        "æƒ…ç·’çˆ†é»å€¼ (1-10)": "æƒ…ç·’çˆ†é»",
        "å§”å±ˆæ²‰é»˜ (1-10)": "å§”å±ˆæ²‰é»˜",
        "èªæ°£å¼·åº¦ (1-10)": "å¼·çƒˆèªæ°£"
    }
    
    # è¨ˆç®—æ¯å€‹æƒ…ç·’é¡åˆ¥çš„å¹³å‡åˆ†æ•¸
    emotion_averages = {}
    for col in emotion_columns:
        emotion_averages[col] = df[col].mean()
    
    # æ‰¾å‡ºå¹³å‡åˆ†æ•¸æœ€é«˜çš„æƒ…ç·’é¡åˆ¥
    top_emotion_col = max(emotion_averages, key=emotion_averages.get)
    top_emotion = emotion_map[top_emotion_col]
    
    # è¨ˆç®—é«˜åˆ†ä¾‹å­çš„æ¯”ä¾‹ (åˆ†æ•¸ >= 7 è¦–ç‚ºé«˜åˆ†)
    high_score_count = sum(1 for score in df[top_emotion_col] if score >= 7)
    top_emotion_percent = round((high_score_count / total_entries) * 100)
    
    # æ‰¾å‡º Top 10 çˆ†é»é‡‘å¥ (æ ¹æ“š NLP ç¸½åˆ†æ’åº)
    top_quotes = df.sort_values("NLP ç¸½åˆ† (1-100)", ascending=False)[["æ¿ƒç¸®é‡‘å¥", "NLP ç¸½åˆ† (1-100)"]].head(10)
    
    # ç”Ÿæˆæ´å¯Ÿ
    # 1. è·å ´æ¬ŠåŠ›è©±è¡“æ¨¡æ¿
    power_pattern = "è·å ´ä¸»ç®¡é€šéè²¬ä»»é¡å£“åŠ›ã€ç¸¾æ•ˆå¨è„…å’Œè³‡æºæ§åˆ¶å½¢æˆå¤šå±¤æ¬¡å¿ƒç†æ“æ§ï¼Œä»¥ã€Œä½¿å‘½å¿…é”ã€ç‚ºåå¼·åˆ¶å“¡å·¥ä¸åˆç†è²·å–®è¡Œç‚ºï¼Œä¸¦è—‰ç”±å®¢æˆ¶è³‡æºæŒæ§èˆ‡ç¸¾æ•ˆè©•æ ¸æ¬ŠåŠ›å½¢æˆä¸å°ç­‰é—œä¿‚ã€‚æœ€å¸¸è¦‹æ‰‹æ³•åŒ…æ‹¬ï¼šå¨è„…ç§»è½‰å®¢æˆ¶ã€æš—ç¤ºè²¸æ¬¾è²·å–®ã€ç•¶çœ¾ç¾è¾±ã€ç¸¾æ•ˆè€ƒæ ¸é€£çµå’Œå¼·åˆ¶é”æ¨™ç­‰ã€‚"
    
    # 2. å…§å¿ƒåå·®å‰–æ
    inner_contrast = "è¡¨å–®é¡¯ç¤ºå¾æ¥­äººå“¡å¤–åœ¨è¢«è¿«æœå¾ã€å…§å¿ƒå¼·çƒˆæŠ—æ‹’çš„çŸ›ç›¾ç‹€æ…‹ã€‚å…¬é–‹å ´åˆä¸æ•¢åæŠ—ï¼Œä½†åŒ¿åè¡¨é”æ™‚æƒ…ç·’æ¥µåº¦æ¿€çƒˆï¼Œå¸¸è¦‹ã€Œå»æ­»ã€ç­‰å¼·çƒˆå­—çœ¼ï¼Œé¡¯ç¤ºé•·æœŸå£“æŠ‘ä¸‹çš„æƒ…ç·’çˆ†ç™¼ï¼Œä»¥åŠå°è·å ´ç’°å¢ƒå¾¹åº•å¤±æœ›çš„æ…‹åº¦ã€‚å¤šæ•¸äººæ„Ÿåˆ°è¢«è¿«é¸æ“‡ã€Œè³ºéŒ¢ç¶­ç”Ÿã€æˆ–ã€Œä¿æœ‰å°Šåš´ã€çš„å…©é›£å›°å¢ƒã€‚"
    
    # 3. æ ¸å¿ƒç—›é»çµ±æ•´
    core_pain = "è­‰åˆ¸èˆ‡éŠ€è¡Œæ¥­IPOéŠ·å”®æ–‡åŒ–å½¢æˆä¸‰å¤§æ ¸å¿ƒå‚·å®³ï¼š(1)å¼·åˆ¶å“¡å·¥è‡ªè²»è³¼è²·ä½¿è²¡å‹™é™·å…¥å›°å¢ƒï¼Œç”šè‡³è¢«è¿«è²¸æ¬¾ï¼›(2)ç¸¾æ•ˆå£“åŠ›è½‰å«é€ æˆåš´é‡å¿ƒç†å¥åº·å•é¡Œï¼Œéƒ¨åˆ†äººå‡ºç¾ç„¦æ…®ã€å¤±çœ ç­‰ç—‡ç‹€ï¼›(3)å®¢æˆ¶è³‡æºè¢«ç•¶ä½œçæ‡²å·¥å…·ï¼Œå°è‡´å“¡å·¥å–ªå¤±è·æ¥­å°Šåš´èˆ‡å°ˆæ¥­è‡ªä¿¡ã€‚ç‰¹åˆ¥æ˜¯è³‡æ·ºèˆ‡å¥³æ€§å¾æ¥­äººå“¡ï¼Œæ›´æ˜“é­é‡ä¸ç•¶å°å¾…ã€‚"
    
    # 4. å¾ŒçºŒè¿½è¹¤å ±å°è§’åº¦
    follow_up_angles = [
        "ã€Œè²¸æ¬¾è²·å–®ã€ç¾è±¡èƒŒå¾Œï¼šé‡‘èæ¥­ç¸¾æ•ˆæ–‡åŒ–çš„åˆ¶åº¦æ€§æš´åŠ›",
        "é‡‘èå¥³æ€§å¾æ¥­äººå“¡çš„å›°å¢ƒï¼šæ€§åˆ¥æ­§è¦–èˆ‡ä¸ç•¶è¨€èªç’°å¢ƒèª¿æŸ¥",
        "ã€Œè¢«è‡ªå·±çš„è–ªæ°´ç¶æ¶ã€ï¼šé‡‘èå¾æ¥­äººå“¡çš„å¿ƒç†å¥åº·å±æ©Ÿèˆ‡è§£æ–¹"
    ]
    
    # ç”Ÿæˆ Markdown å ±å‘Š
    markdown_content = f"""## ğŸ“Š é—œéµçµ±è¨ˆ  
- **å—è¨ªç­†æ•¸**ï¼š{total_entries}  
- **å¹³å‡ NLP ç¸½åˆ†**ï¼š{avg_score} åˆ†  
- **æœ€é«˜ï¼æœ€ä½åˆ†ç¯„ä¾‹**ï¼š  
  - æœ€é«˜åˆ†ï¼šæ¿ƒç¸®é‡‘å¥ã€Œ{highest_score_row['æ¿ƒç¸®é‡‘å¥']}ã€ ({int(highest_score_row['NLP ç¸½åˆ† (1-100)'])} åˆ†)  
  - æœ€ä½åˆ†ï¼šæ¿ƒç¸®é‡‘å¥ã€Œ{lowest_score_row['æ¿ƒç¸®é‡‘å¥']}ã€ ({int(lowest_score_row['NLP ç¸½åˆ† (1-100)'])} åˆ†)  
- **æœ€å¸¸è¦‹æƒ…ç·’åˆ†é¡**ï¼š{top_emotion} ({top_emotion_percent}%)

## ğŸ”¥ Top 10 çˆ†é»é‡‘å¥  
"""
    
    # åŠ å…¥ Top 10 çˆ†é»é‡‘å¥
    for i, (_, row) in enumerate(top_quotes.iterrows(), 1):
        if i <= 10:
            markdown_content += f"{i}. ã€Œ{row['æ¿ƒç¸®é‡‘å¥']}ã€ â€” ç¸½åˆ† {int(row['NLP ç¸½åˆ† (1-100)'])}  \n"
    
    # åŠ å…¥æ·±åº¦æ´å¯Ÿèˆ‡å ±å°å»ºè­°
    markdown_content += f"""
## ğŸ•µï¸â€â™‚ï¸ æ·±åº¦æ´å¯Ÿèˆ‡å ±å°å»ºè­°  
1. **è·å ´æ¬ŠåŠ›è©±è¡“æ¨¡æ¿**ï¼š{power_pattern}  
2. **å…§å¿ƒåå·®å‰–æ**ï¼š{inner_contrast}  
3. **æ ¸å¿ƒç—›é»çµ±æ•´**ï¼š{core_pain}  
4. **å¾ŒçºŒè¿½è¹¤å ±å°è§’åº¦**ï¼š  
   - {follow_up_angles[0]}  
   - {follow_up_angles[1]}  
   - {follow_up_angles[2]}  
"""
    
    # è¼¸å‡º Markdown æª”æ¡ˆ
    with open("IPO.md", "w", encoding="utf-8") as f:
        f.write(markdown_content)

if __name__ == '__main__':
    main()